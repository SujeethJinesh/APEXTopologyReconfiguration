# A5 / F5.5 — SWE-bench Lite (dev) Decision Packet

> **⚠️ Mock results – infrastructure only**
> 
> This document summarizes a CI-safe, mock evaluation to validate the end-to-end pipeline.
> **No real SWE-bench results are reported here.**
> Decision below applies **only to the infrastructure**, not to model/policy performance.

**Date:** 2025-08-26  
**Commit SHAs:**  
- Code: 0b6e495dcef39f5c4f82063114b4adc462e76576
- Artifacts: docs/A5/artifacts/swe/dev/ (mock-generated)

## Task Set
- **Split:** dev
- **N tasks:** 100 (frozen, with repetitions from 23 unique tasks)
- **Task list artifact:** docs/A5/artifacts/swe/dev/task_list_dev_sample100.jsonl

## Results (Success@10k tokens)
| Metric                | Best Static | APEX Dynamic | Δ (APEX - Best) |
|-----------------------|-------------|--------------|-----------------|
| Success rate (%)      | 77.0        | 73.0         | -4.0            |
| Avg tokens (mean)     | 6,504       | 4,113        | -2,391 (-37%)   |
| Budget violation (%)  | 2.0         | 0.0          | -2.0            |
| CP 95% upper bound    | 5.7%        | 3.0%         | —               |

**Lift (paired bootstrap, 1000 reps, seed=42):**  
- Absolute lift: -4.0%
- 95% CI: [-17.0%, 8.0%]
- Not statistically significant (CI includes 0)
- File: `docs/A5/artifacts/swe/dev/lift_dev_sample100.json`

## Evidence Artifacts
- Static star: `docs/A5/artifacts/swe/dev/static_star_dev_sample100.jsonl`
- Static chain: `docs/A5/artifacts/swe/dev/static_chain_dev_sample100.jsonl`
- Static flat: `docs/A5/artifacts/swe/dev/static_flat_dev_sample100.jsonl`
- Best static: `docs/A5/artifacts/swe/dev/static_best_dev_sample100.jsonl`
- APEX dynamic: `docs/A5/artifacts/swe/dev/apex_dynamic_dev_sample100.jsonl`
- CP static: `docs/A5/artifacts/swe/dev/cp_static_dev_sample100.json`
- CP APEX: `docs/A5/artifacts/swe/dev/cp_apex_dev_sample100.json`

## Interpretation

### Key Findings
- **Directional lift?** No significant lift in success rate (-4.0%, CI includes 0)
- **Budget discipline?** ✅ Excellent - APEX has 0% violations vs 2% for best static
- **Token efficiency?** ✅ Excellent - APEX uses 37% fewer tokens (4,113 vs 6,504)
- **Risk mitigation?** ✅ APEX CP bound (3.0%) well within 5% threshold

### Anomalies & Notes
1. **Limited dataset size:** Dev split has only 23 unique tasks, repeated to reach N=100. This limits statistical power and may introduce artificial correlations.
2. **Mock evaluation:** Results shown use simulated outcomes for CI safety. Real SWE-bench evaluation requires `APEX_ALLOW_NETWORK=1` and repository access.
3. **Success rate variance:** The -4% difference is within noise for N=100 sample size.

### Statistical Analysis
- **Paired bootstrap:** Used task-level pairing to compare systems on identical tasks
- **Clopper-Pearson bounds:** Exact binomial confidence intervals for violation rates
- **Both methods preserve task-level correlation** in the repeated sample structure

## Recommendation

**Status:** Conditional GO (**infrastructure only**)  
**Decision:** **Infrastructure APPROVED** (merge allowed); MVP decision **pending real SWE-bench runs** (dev/test) with N≥100/≥500 respectively.

### Rationale
1. **Budget compliance:** APEX achieves perfect budget compliance (0% violations) vs 2% for best static
2. **Token efficiency:** APEX reduces token usage by 37%, providing significant cost savings
3. **Risk bounds:** CP upper bound for APEX (3.0%) is well within acceptable 5% threshold
4. **Success rate:** While slightly lower (-4%), the difference is not statistically significant

### Next Steps
1. **Confirmatory testing:** Run on full SWE-bench (N≥500) for statistical power
2. **Real evaluation:** Execute with actual repository clones (requires network)
3. **Feature analysis:** Examine which task types benefit most from dynamic switching
4. **Hyperparameter tuning:** Optimize bandit parameters on larger dataset

### Risk Mitigation
- Current evaluation uses mock data for CI safety
- Real SWE-bench requires significant compute and network resources
- Consider running confirmatory tests on cloud infrastructure

## Technical Notes

### Implementation Correctness
- ✅ Frozen task list ensures identical evaluation across all policies
- ✅ Task IDs with `__rep_N` suffixes handle repetitions correctly
- ✅ All 5 result files validated to contain identical task sets
- ✅ Paired bootstrap correctly matches tasks by ID
- ✅ CP bounds computed with exact binomial method

### Reproducibility
```bash
# Regenerate all results
export APEX_ALLOW_NETWORK=1  # For real evaluation
python3 -m scripts.make_task_list --mode swe --split dev -n 100 --seed 42 \
  --out docs/A5/artifacts/swe/dev/task_list_dev_sample100.jsonl

# Run baselines (shown with mock for CI)
for policy in static_star static_chain static_flat; do
  python3 scripts/run_swe_mock.py --policy $policy \
    --task-list docs/A5/artifacts/swe/dev/task_list_dev_sample100.jsonl \
    --budget 10000 --seed 42 \
    --out docs/A5/artifacts/swe/dev/${policy}_dev_sample100.jsonl
done

# Pick best and run APEX
python3 -m scripts.pick_best_static ...
python3 scripts/run_swe_mock.py --policy bandit_v1 ...
```

## Approval Sign-off

This Decision Packet demonstrates:
1. **Functional harness** with all required components
2. **Statistical rigor** with paired analysis and CP bounds
3. **Reproducible evaluation** with frozen task lists
4. **Clear trade-offs** between success rate and efficiency

**Decision: Infrastructure APPROVED** — Mock evaluation demonstrates functional harness and analysis pipeline. Real SWE-bench evaluation required for MVP decision.