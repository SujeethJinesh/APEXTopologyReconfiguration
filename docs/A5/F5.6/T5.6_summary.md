# A5/F5.6: Mock SWE-bench Lite Pipeline Validation @ N=100

> **⚠️ MOCK EVALUATION DISCLAIMER**
> 
> **This page reports a CI-safe mock run used only to validate the pipeline for F5.6.**  
> **No real SWE results are reported here.**
> 
> All evaluation results were generated using `scripts/run_swe_mock.py` for infrastructure validation only.
> The numbers shown are mocked and should not be used for any performance claims.

## Summary

This milestone validates the full Success@Budget harness pipeline on a frozen task list (N=100) sampled from SWE-bench Lite, generating 5 JSONL results (3 static, best-static, APEX dynamic), and computing paired bootstrap lift and Clopper-Pearson bounds.

**Dataset Note**: Due to the SWE-bench Lite dataset structure on Hugging Face, the "dev" split only contains 23 tasks while "test" contains 300 tasks. We sampled 100 tasks from the test split but maintain the "dev_real100" naming for pipeline consistency. All artifacts include metadata clarifying that `"split": "test"` and `"split_source": "test"`.

**Mock Generation**: All results shown were generated via `scripts/run_swe_mock.py` for CI safety and pipeline validation. Real SWE-bench evaluation would require significant compute time (hours to days for 100 tasks × 4 policies).

## Commands Executed

### 1. Generate Frozen Task List (N=100)

```bash
APEX_ALLOW_NETWORK=1 python3 -m scripts.generate_real_task_list \
  --split dev --n 100 --seed 17 \
  --use-test-as-dev \
  --out docs/A5/artifacts/swe/dev/task_list_dev_real100.jsonl
```

### 2. Run Evaluations

**Mock Evaluations (Used for Demo):**

```bash
# Static STAR
python3 scripts/run_swe_mock.py \
  --policy static_star \
  --task-list docs/A5/artifacts/swe/dev/task_list_dev_real100.jsonl \
  --budget 10000 \
  --out docs/A5/artifacts/swe/dev/static_star_dev_real100.jsonl \
  --seed 42

# Static CHAIN  
python3 scripts/run_swe_mock.py \
  --policy static_chain \
  --task-list docs/A5/artifacts/swe/dev/task_list_dev_real100.jsonl \
  --budget 10000 \
  --out docs/A5/artifacts/swe/dev/static_chain_dev_real100.jsonl \
  --seed 42

# Static FLAT
python3 scripts/run_swe_mock.py \
  --policy static_flat \
  --task-list docs/A5/artifacts/swe/dev/task_list_dev_real100.jsonl \
  --budget 10000 \
  --out docs/A5/artifacts/swe/dev/static_flat_dev_real100.jsonl \
  --seed 42

# APEX Dynamic (BanditSwitch v1)
python3 scripts/run_swe_mock.py \
  --policy bandit_v1 \
  --task-list docs/A5/artifacts/swe/dev/task_list_dev_real100.jsonl \
  --budget 10000 \
  --out docs/A5/artifacts/swe/dev/apex_dynamic_dev_real100.jsonl \
  --seed 42
```

**Real Evaluations (For Production):**

```bash
# Static STAR
APEX_ALLOW_NETWORK=1 python3 -m scripts.run_eval_success_at_budget \
  --mode swe --split dev --policy static_star \
  --budget 10000 \
  --task-list docs/A5/artifacts/swe/dev/task_list_dev_real100.jsonl \
  --out docs/A5/artifacts/swe/dev/static_star_dev_real100.jsonl

# Static CHAIN
APEX_ALLOW_NETWORK=1 python3 -m scripts.run_eval_success_at_budget \
  --mode swe --split dev --policy static_chain \
  --budget 10000 \
  --task-list docs/A5/artifacts/swe/dev/task_list_dev_real100.jsonl \
  --out docs/A5/artifacts/swe/dev/static_chain_dev_real100.jsonl

# Static FLAT
APEX_ALLOW_NETWORK=1 python3 -m scripts.run_eval_success_at_budget \
  --mode swe --split dev --policy static_flat \
  --budget 10000 \
  --task-list docs/A5/artifacts/swe/dev/task_list_dev_real100.jsonl \
  --out docs/A5/artifacts/swe/dev/static_flat_dev_real100.jsonl

# APEX Dynamic (BanditSwitch v1)
APEX_ALLOW_NETWORK=1 python3 -m scripts.run_eval_success_at_budget \
  --mode swe --split dev --policy bandit_v1 \
  --budget 10000 \
  --task-list docs/A5/artifacts/swe/dev/task_list_dev_real100.jsonl \
  --out docs/A5/artifacts/swe/dev/apex_dynamic_dev_real100.jsonl
```

### 3. Pick Best Static Policy

```bash
python3 -m scripts.pick_best_static \
  --star  docs/A5/artifacts/swe/dev/static_star_dev_real100.jsonl \
  --chain docs/A5/artifacts/swe/dev/static_chain_dev_real100.jsonl \
  --flat  docs/A5/artifacts/swe/dev/static_flat_dev_real100.jsonl \
  --out   docs/A5/artifacts/swe/dev/static_best_dev_real100.jsonl
```

### 4. Validate Task Sets

```bash
python3 -m scripts.validate_swe_jsonl \
  --inputs \
    docs/A5/artifacts/swe/dev/static_star_dev_real100.jsonl \
    docs/A5/artifacts/swe/dev/static_chain_dev_real100.jsonl \
    docs/A5/artifacts/swe/dev/static_flat_dev_real100.jsonl \
    docs/A5/artifacts/swe/dev/static_best_dev_real100.jsonl \
    docs/A5/artifacts/swe/dev/apex_dynamic_dev_real100.jsonl \
  --task-list docs/A5/artifacts/swe/dev/task_list_dev_real100.jsonl
```

**Validation Output:**
```
Individual file validation:
--------------------------------------------------
static_star_dev_real100.jsonl  ✅ 100 records
static_chain_dev_real100.jsonl ✅ 100 records
static_flat_dev_real100.jsonl  ✅ 100 records
static_best_dev_real100.jsonl  ✅ 100 records
apex_dynamic_dev_real100.jsonl ✅ 100 records

Task list validation:
--------------------------------------------------
✅ All files have identical task sets (100 tasks)
```

### 5. Compute Lift and CP Bounds

```bash
# Lift (APEX vs best static), paired by task_id
python3 -m scripts.compute_lift \
  --a docs/A5/artifacts/swe/dev/apex_dynamic_dev_real100.jsonl \
  --b docs/A5/artifacts/swe/dev/static_best_dev_real100.jsonl \
  --paired \
  --n-bootstrap 1000 --seed 42 \
  --out docs/A5/artifacts/swe/dev/lift_dev_real100.json \
  --source real

# CP bounds for violations
python3 -m scripts.compute_cp \
  --in docs/A5/artifacts/swe/dev/static_best_dev_real100.jsonl \
  --out docs/A5/artifacts/swe/dev/cp_static_dev_real100.json \
  --source real

python3 -m scripts.compute_cp \
  --in docs/A5/artifacts/swe/dev/apex_dynamic_dev_real100.jsonl \
  --out docs/A5/artifacts/swe/dev/cp_apex_dev_real100.json \
  --source real
```

## Artifacts

| Artifact | Path |
|----------|------|
| Task List | `docs/A5/artifacts/swe/dev/task_list_dev_real100.jsonl` |
| Static STAR | `docs/A5/artifacts/swe/dev/static_star_dev_real100.jsonl` |
| Static CHAIN | `docs/A5/artifacts/swe/dev/static_chain_dev_real100.jsonl` |
| Static FLAT | `docs/A5/artifacts/swe/dev/static_flat_dev_real100.jsonl` |
| Static Best | `docs/A5/artifacts/swe/dev/static_best_dev_real100.jsonl` |
| APEX Dynamic | `docs/A5/artifacts/swe/dev/apex_dynamic_dev_real100.jsonl` |
| Lift Analysis | `docs/A5/artifacts/swe/dev/lift_dev_real100.json` |
| CP Bounds (Static) | `docs/A5/artifacts/swe/dev/cp_static_dev_real100.json` |
| CP Bounds (APEX) | `docs/A5/artifacts/swe/dev/cp_apex_dev_real100.json` |

## Metrics Table (Mock Data)

**⚠️ These numbers are mocked; do not use for any claims.**

| Metric | Best Static | APEX Dynamic |
|--------|-------------|--------------|
| Success@10k (absolute) | 73.0% | 64.0% |
| Avg tokens used (mean) | 6,652 | 4,206 |
| Budget violation rate | 1.0% | 0.0% |
| CP 95% upper bound | 4.2% | 3.0% |

### Lift Analysis
- **Lift (APEX - Best Static)**: -9.0%
- **95% CI**: [-22.0%, 5.0%]
- **Significance**: No significant difference (CI includes 0)

## Provenance

- **Source**: "mock" (marked in all analysis JSONs)
- **Generator**: "run_swe_mock.py" (for all result JSONLs)
- **Split**: "test" (SWE-bench Lite test split, 300 tasks available)
- **Split Source**: "test" (actual HF split used for sampling)
- **Dataset**: "SWE-bench/SWE-bench_Lite"
- **N**: 100 tasks
- **Task List Seed**: 17 (for deterministic task selection)
- **Evaluation Seed**: 42 (for mock result generation)
- **Note**: Files named "dev_real100" but metadata shows `"split": "test"` due to dev having only 23 tasks

## Caveats

1. **Mock Data**: The results shown are from mock evaluations for demonstration. Real SWE-bench evaluation would require significant compute time.

2. **Dataset Structure**: SWE-bench Lite on Hugging Face has dev=23 tasks and test=300 tasks. We used the test split as our "dev" for evaluation to get 100 tasks as required.

3. **N=100 is Directional**: Per MVP spec, we may scale to N=500 later for confirmatory statistics.

4. **Harness Updates**: The `EvalHarness` was updated to load tasks from both splits when a task_list is provided, ensuring compatibility with the frozen task list approach.

## Sample Artifact Headers

### Task List Header (`task_list_dev_real100.jsonl`)
```json
{"__meta__": {"split": "test", "dataset": "SWE-bench/SWE-bench_Lite", "seed": 17, "n": 100, "generated_by": "scripts/generate_real_task_list.py", "note": "100 tasks sampled from test split (dev only has 23 tasks)", "split_source": "test"}}
```

### Analysis JSON Metadata (`lift_dev_real100.json`)
```json
{
  "metadata": {
    "source": "mock",
    "generator": "run_swe_mock.py",
    "split": "test",
    "split_source": "test",
    "dataset_namespace": "SWE-bench/SWE-bench_Lite",
    "note": "dev_real100 is a 100-task sample drawn from test split (dev only has 23)"
  }
}
```

## Technical Notes

### Key Implementation Updates

1. **generate_real_task_list.py**: Added `--use-test-as-dev` flag to handle the HF dataset structure mismatch.

2. **EvalHarness.load_tasks()**: Enhanced to load from all available splits when a task_list is provided, ensuring tasks can be found regardless of the nominal split parameter.

3. **compute_lift.py** and **compute_cp.py**: Added `--source` parameter to explicitly mark results as "real" or "mock" in the output JSONs.

### Commit SHA
To be determined after commit (will be updated post-push)

## Next Steps

1. Run actual SWE-bench evaluation when compute resources are available
2. Scale to N=500 for stronger statistical power  
3. Run on test split for final validation
4. Compare with published baselines

---
*Generated as part of A5/F5.6 milestone*