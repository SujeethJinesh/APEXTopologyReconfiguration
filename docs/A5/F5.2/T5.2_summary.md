# A5/F5.2: SWE-bench Lite Integration for Success@Budget Harness

## What We Built

### Provider Layer (`apex/eval/providers/swe_lite.py`)
- `SWELiteProvider`: Loads SWE-bench Lite tasks from Hugging Face or local cache
- Maps fields correctly: `instance_id` → `task_id`, `problem_statement` → `problem`
- Supports both dev (23 tasks) and test (300 tasks) splits
- Falls back to local JSONL when network unavailable (CI-safe)

### Repository Manager (`apex/eval/repo_manager.py`)
- `RepoManager`: Handles git checkout, patch application, test execution
- Caches checkouts by `<org>__<repo>@<commit>` to avoid redundant clones
- Applies test patches via `git apply --index --whitespace=fix`
- Runs tests with deterministic timeout (120s default)
- Supports GitHub token authentication for higher rate limits

### Harness Integration (`apex/eval/harness.py`)
- Added `mode="swe"` support alongside existing `mode="stub"`
- `_run_swe_episode()`: Full SWE task execution with repo checkout
- Tracks topology trace, switches, budget denials
- Success criterion: tests pass AND under budget (Success@Budget)
- Emits extended JSONL with SWE-specific fields

### CLI Updates (`scripts/run_eval_success_at_budget.py`)
- Added `--mode {stub,swe}` flag (default: stub for CI)
- Added `--split {dev,test}` for SWE dataset selection
- Network gated by `APEX_ALLOW_NETWORK=1` environment variable
- Supports `apex_dynamic` policy for SWE mode
- Produces JSONL artifacts with full execution traces

## How to Run

### CI-Safe Local Smoke Test (Stub Mode)
```bash
# No network required - uses deterministic stub tasks
python -m scripts.run_eval_success_at_budget \
  --mode=stub \
  --episodes=12 \
  --budget=10000 \
  --policy=apex_dynamic \
  --out docs/A5/artifacts/stub_smoke.jsonl
```

### SWE-bench Lite Dev Sample (Requires Network)
```bash
# Set GitHub token for better rate limits (optional)
export GITHUB_TOKEN=ghp_your_token_here

# Enable network and run small dev sample
APEX_ALLOW_NETWORK=1 \
python -m scripts.run_eval_success_at_budget \
  --mode=swe \
  --split=dev \
  --episodes=10 \
  --budget=10000 \
  --policy=apex_dynamic \
  --out docs/A5/artifacts/swe/dev/apex_dynamic_10.jsonl
```

### Compare Static Policies
```bash
# Run all static policies on same tasks
for policy in static_star static_chain static_flat; do
  APEX_ALLOW_NETWORK=1 \
  python -m scripts.run_eval_success_at_budget \
    --mode=swe \
    --split=dev \
    --episodes=10 \
    --budget=10000 \
    --policy=$policy \
    --out docs/A5/artifacts/swe/dev/${policy}_10.jsonl
done
```

### Compute Lift from Artifacts
```bash
# Paired bootstrap CI for lift computation
python -m scripts.compute_lift \
  docs/A5/artifacts/swe/dev/apex_dynamic_10.jsonl \
  docs/A5/artifacts/swe/dev/static_star_10.jsonl \
  --paired \
  --n-bootstrap 1000
```

## JSONL Output Schema

Each episode produces one JSONL line with:

```json
{
  "task_id": "django__django-12345",
  "policy": "apex_dynamic",
  "budget": 10000,
  "tokens_used": 7234,
  "budget_denied": 0,
  "success": false,
  "topology_trace": [
    {"tick": 0, "topo": "star", "dwell": 1, "cooldown": 0},
    {"tick": 1, "topo": "chain", "dwell": 1, "cooldown": 3}
  ],
  "switches": 1,
  "episode_ms": 1234.56
}
```

## Tests

All tests are CI-safe (no network by default):

```bash
# Provider schema validation
pytest tests/test_swe_provider_schema.py -v

# Repository manager and harness wiring
pytest tests/test_swe_mode_wiring.py -v

# Skip networked tests (marked with @pytest.mark.external)
pytest tests/ -v -m "not external"
```

## Limitations

**Current Status (MVP):**
- Token counting is simulated (not from actual LLM calls)
- Agent behavior is randomized (not solving real tasks yet)
- Success rates will be very low (~0-5%) initially
- No caching of test results between runs
- No parallel execution (episodes run serially)

**This is expected** - we're establishing the evaluation mechanics first. The next milestone (F5.3) will focus on improving agent behaviors to achieve meaningful success rates.

## Why This Scope

This implementation:
1. **Preserves all CI stability** - stub mode remains default, no network in CI
2. **Enables real dataset evaluation** - actual SWE-bench Lite tasks, not synthetic
3. **Maintains paired bootstrap discipline** - task IDs enable valid lift computation
4. **Produces machine-verifiable artifacts** - all metrics computed from JSONL
5. **Sets foundation for agent improvements** - harness ready for real problem-solving

## Dataset References

- **SWE-bench Lite Overview**: https://www.swebench.com/lite.html
  - 300-task curated subset of full SWE-bench
  - Faster iteration and evaluation
  - Same task format and evaluation protocol

- **Hugging Face Dataset**: https://huggingface.co/datasets/princeton-nlp/SWE-bench_Lite
  - Official dataset with dev (23) and test (300) splits
  - Field definitions and schema documentation
  - Direct download via `datasets` library

## SWE-bench Lite Runbook

### Initial Setup
1. Install datasets library: `pip install datasets`
2. Set GitHub token: `export GITHUB_TOKEN=ghp_...`
3. Create cache directory: `mkdir -p ~/.cache/apex/swe_bench`

### Running Evaluations
1. Start with dev split (23 tasks) for quick iteration
2. Use `--episodes` flag to limit scope during testing
3. Monitor `episode_ms` to track execution time
4. Check `topology_trace` for switching behavior
5. Use `compute_lift.py` for paired comparison

### Troubleshooting
- **"Dataset not found"**: Set `APEX_ALLOW_NETWORK=1` or pre-download JSONL
- **"Repository clone failed"**: Check GitHub token and network connectivity
- **"Test patch failed"**: Some patches may not apply cleanly (expected)
- **"Tests timeout"**: Increase timeout in RepoManager (default 120s)

---

*Implementation complete. Ready for real SWE-bench Lite evaluation.*